{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.autograd import Variable, profiler\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "words = []\n",
    "for sentence in corpus:\n",
    "    for word in sentence.split():\n",
    "         if word not in words:\n",
    "            words.append(word)\n",
    "        \n",
    "word2idx = {w:idx for (idx, w) in enumerate(words)}\n",
    "idx2word = {idx:w for (idx, w) in enumerate(words)}\n",
    "\n",
    "vocabulary_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    word_vec_one_hot = np.zeros(vocabulary_size)\n",
    "    word_vec_one_hot[word2idx[word]] = 1\n",
    "    return word_vec_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 10\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        indices = [word2idx[w] for w in words]\n",
    "        for i in range(len(indices)):\n",
    "            # center word, context\n",
    "            # i is center word index\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_idx = i + w\n",
    "                if context_idx < 0 or context_idx >= len(indices) or i == context_idx:\n",
    "                    continue\n",
    "                center_vec_one_hot = np.zeros(vocabulary_size)\n",
    "                center_vec_one_hot[indices[i]] = 1\n",
    "                \n",
    "                context_idx = indices[context_idx]\n",
    "                                \n",
    "                yield center_vec_one_hot, context_idx\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.496821568693433\n",
      "2.0961487259183613\n",
      "1.7688039268766131\n",
      "1.6510580139500755\n",
      "1.6036454788276127\n",
      "1.5816143589360374\n"
     ]
    }
   ],
   "source": [
    "# Network definition\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "\n",
    "\n",
    "for epo in range(501):\n",
    "    avg_loss = 0\n",
    "    samples = 0\n",
    "    for data, target in train_generator():\n",
    "        x = Variable(torch.from_numpy(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "        \n",
    "        samples += len(y_true)\n",
    "        \n",
    "        a1 = torch.matmul(W1, x)\n",
    "        a2 = torch.matmul(W2, a1)\n",
    "\n",
    "        log_softmax = F.log_softmax(a2, dim=0)\n",
    "\n",
    "        network_pred_dist = F.softmax(log_softmax, dim=0)\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        W1.data -= 0.002 * W1.grad.data\n",
    "        W2.data -= 0.002 * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "        \n",
    "    \n",
    "    if epo % 100 == 0:\n",
    "        print(avg_loss / samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scikitplot.decomposition import plot_pca_2d_projection\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(W1.data.numpy().T)\n",
    "proj = pca.transform(W1.data.numpy().T)\n",
    "ax = plot_pca_2d_projection(pca, W1.data.numpy().T, np.array(words), feature_labels=words, figsize=(12,12), text_fontsize=12)\n",
    "# ax.legend(None)\n",
    "for i, txt in enumerate(words):\n",
    "    ax.annotate(txt, (proj[i,0], proj[i,1]), size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector_v(word):\n",
    "    return W1[:, word2idx[word]].data.numpy()\n",
    "\n",
    "def get_word_vector_u(word):\n",
    "    return W2[word2idx[word],:].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland to Warsaw is like Germany to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = 1 * get_word_vector_v('poland') + 1 * get_word_vector_u('poland')\n",
    "ger = 1 * get_word_vector_v('germany') + 1 * get_word_vector_u('germany') \n",
    "waw = 1 * get_word_vector_v('warsaw') + 1 * get_word_vector_u('warsaw') \n",
    "\n",
    "yyy = waw - pol + ger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "distances = [(v, cosine(yyy, 1 * get_word_vector_u(v) + 1 * get_word_vector_v(v))) for v in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland to Warsaw is like Germany to Berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 0.8570210635662079),\n",
       " ('is', 1.0280788782984018),\n",
       " ('a', 1.1927961856126785),\n",
       " ('king', 1.1952263414859772),\n",
       " ('she', 1.0590651705861092),\n",
       " ('queen', 0.6953186988830566),\n",
       " ('man', 0.8267989605665207),\n",
       " ('woman', 1.4676547944545746),\n",
       " ('warsaw', 0.35097044706344604),\n",
       " ('poland', 1.1727249324321747),\n",
       " ('capital', 0.9477489851415157),\n",
       " ('berlin', 0.2761566638946533),\n",
       " ('germany', 0.28735482692718506),\n",
       " ('paris', 1.1595857292413712),\n",
       " ('france', 1.3894160687923431)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In what context Paris appears?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he: 0.01\n",
      "is: 0.45\n",
      "a: 0.03\n",
      "king: 0.01\n",
      "she: 0.01\n",
      "queen: 0.00\n",
      "man: 0.02\n",
      "woman: 0.00\n",
      "warsaw: 0.00\n",
      "poland: 0.03\n",
      "capital: 0.00\n",
      "berlin: 0.00\n",
      "germany: 0.01\n",
      "paris: 0.00\n",
      "france: 0.42\n"
     ]
    }
   ],
   "source": [
    "context_to_predict = get_word_vector_v('paris')\n",
    "hidden = Variable(torch.from_numpy(context_to_predict)).float()\n",
    "a = torch.matmul(W2, hidden)\n",
    "probs = F.softmax(a, dim=0).data.numpy()\n",
    "for context, prob in zip(words, probs):\n",
    "    print(f'{context}: {prob:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp] *",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
